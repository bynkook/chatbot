# chat_app_V2.1.py

import os
import json
import re
import time
# import pandas as pd   # only for table expression printing
import streamlit as st
from typing import List, Dict, Optional
from llama_cpp import Llama
# user functions
from model_bundle import ModelBundle
from predict_parser import (
    parse_predict_message,
    parse_predict_natural,
    build_missing_prompt,
    REQUIRED_INPUT,
    is_predict_intent,
)

st.set_page_config(page_title="BKChat Local", layout="wide")
st.markdown("""
<style>
/* ì „ì²´ íƒ€ì´í¬/ë§í¬ */
html, body, [class*="block-container"] { font-family: 'Inter', system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif; }
a { color:#9E7BFF !important; }

/* ì¹´ë“œ/ì»¨í…Œì´ë„ˆ ì—¬ë°± */
.stMainBlockContainer { padding-top: 1rem; }

/* ì…ë ¥ì°½, select, number input */
div[data-baseweb="input"] > div, div[role="spinbutton"], textarea {
  background:#0F1531 !important; border:1px solid #283059 !important; color:#E6E6F0 !important; border-radius:10px;
}
/* ìŠ¬ë¼ì´ë” */
div.stSlider > div[data-baseweb="slider"] > div {
    background-color: transparent !important;
}

/* Slider ì§„í–‰ ë¶€ë¶„ ìƒ‰ìƒ ë³€ê²½ (ì˜¤ë Œì§€) */
div.stSlider > div[data-baseweb="slider"] > div > div {
    background-color: #FFA500 !important;
}

/* Slider í•¸ë“¤ ìƒ‰ìƒ ë³€ê²½ (ì˜¤ë Œì§€) */
div.stSlider [role="slider"] {
    background-color: #FF8C00 !important;
    border: 2px solid #FF8C00 !important;
}

/* ë²„íŠ¼ */
button[kind="primary"] {
  background: linear-gradient(135deg,#7C4DFF 0%, #5B8CFF 100%) !important;
  color:#fff !important; border:0 !important; border-radius:12px !important;
}
button[kind="secondary"] {
  background:#1B2246 !important; color:#E6E6F0 !important; border:1px solid #2A3370 !important; border-radius:12px !important;
}
button:hover { filter:brightness(1.05); }

/* ì‚¬ì´ë“œë°” */
section[data-testid="stSidebar"] {
  background:#0C1230 !important; border-right:1px solid #202A55;
}

/* ì±„íŒ… ë²„ë¸” */
div[data-testid="stChatMessage"] {
  background:#10173A; border:1px solid #263066; border-radius:14px; padding:12px 14px; box-shadow: 0 6px 20px rgba(0,0,0,.25);
}
div[data-testid="stChatMessage"][data-testid="stChatMessage"] p { color:#E6E6F0; }
/* user/assistant ì•„ì´ì½˜ ìƒ‰ ëŒ€ë¹„ */
div[data-testid="stChatMessage"] svg { fill:#9E7BFF !important; }

/* ìŠ¤í¬ë¡¤ë°” */
::-webkit-scrollbar { width:10px; height:10px; }
::-webkit-scrollbar-thumb { background:#2A3370; border-radius:8px; }
::-webkit-scrollbar-track { background:#0B1026; }
</style>
""", unsafe_allow_html=True)

# ---------- helpers ----------
def auto_chat_format(model_path: str, user_choice: str) -> Optional[str]:
    if user_choice == "auto":  # íŒŒì¼ëª…ìœ¼ë¡œ ì¶”ì •
        name = os.path.basename(model_path).lower()
        if "qwen" in name or "deepseek" in name: return "qwen"
        if "llama-3" in name or "llama3" in name: return "llama-3"
        return None
    if user_choice == "none": return None
    return user_choice

def strip_think(text: str) -> str:
    # í•„ìš” ì‹œ ì²´í¬ë°•ìŠ¤ë¡œ On/Off
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()

@st.cache_data(show_spinner=False, max_entries = 4)
def load_bundle_cached(path: str) -> ModelBundle:
    '''joblib ë²ˆë“¤ì„ ë””ìŠ¤í¬ì— ë¡œë“œí•˜ê³  ìºì‹œ'''
    return ModelBundle.load(path)

@st.cache_resource(show_spinner="Loading model...")
def load_llm(model_path: str, n_ctx: int, n_threads: int, chat_format: Optional[str]) -> Llama:
    try:
        return Llama(model_path=model_path, n_ctx=n_ctx, n_threads=n_threads,
                     chat_format=chat_format, verbose=False)
    except Exception as e:
        if "Invalid chat handler" in str(e):
            return Llama(model_path=model_path, n_ctx=n_ctx, n_threads=n_threads,
                         chat_format=None, verbose=False)
        raise

def _split_reasoning(text: str) -> tuple[str, str]:
    """ëª¨ë¸ ì‘ë‹µì—ì„œ <think> ë¸”ë¡ì„ ë¶„ë¦¬í•œë‹¤. visible, think ë¥¼ ë°˜í™˜."""
    thinks = re.findall(r"<think>(.*?)</think>", text, flags=re.DOTALL)
    think_text = "\n\n".join(t.strip() for t in thinks) if thinks else ""
    visible = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
    return visible, think_text

def chat_once(llm: Llama, messages: List[Dict[str,str]], temperature: float, top_p: float, max_tokens: int, stream_placeholder):
    # ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
    try:
        it = llm.create_chat_completion(messages=messages, temperature=temperature, top_p=top_p, max_tokens=max_tokens, stream=True)
        buf=[]
        for ch in it:
            delta = ch["choices"][0]["delta"].get("content","")
            if delta:
                buf.append(delta)
                stream_placeholder.markdown("".join(buf))
        raw = "".join(buf)
        return _split_reasoning(raw)
    except TypeError:
        resp = llm.create_chat_completion(messages=messages, temperature=temperature, top_p=top_p, max_tokens=max_tokens)
        raw = resp["choices"][0]["message"]["content"]
        stream_placeholder.markdown(raw)
        return _split_reasoning(raw)

# ---------- sidebar ----------
with st.sidebar:
    st.header("âš™ï¸ Settings")
    model_path = st.text_input("Model (.gguf) path", r"c:\Users\BKHOME\mycode\chatbot\models\Qwen3-4B-Instruct-2507-Q3_K_S.gguf")
    chat_fmt_choice = st.selectbox("chat_format", ["auto","qwen","llama-3","none"], index=0)
    ctx = st.number_input("n_ctx", 256, 8192, 2048, 256)
    threads = st.number_input("n_threads", 1, 64, max(1,(os.cpu_count() or 4)-1), 1)
    temp = st.slider("temperature", 0.0, 1.5, 0.6, 0.05)
    topp = st.slider("top_p", 0.1, 1.0, 0.95, 0.01)
    toks = st.number_input("max_tokens", 16, 4096, 512, 16)
    # ìƒˆ ë©”ì‹œì§€ì—ë§Œ ì ìš©ë˜ëŠ” ê¸°ë³¸ ì ‘í˜ ì˜µì…˜
    hide_think_default = st.checkbox("Hide reasoning by default", value=True, help="ë³€ê²½ì€ ìƒˆ ì‘ë‹µì—ë§Œ ì ìš©ë©ë‹ˆë‹¤.")
    reload_btn = st.button("Reload model", width="stretch")
    st.divider()
    sys_default = "ë‹¹ì‹ ì€ í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µí•˜ëŠ” ì¡°ìˆ˜ì…ë‹ˆë‹¤."
    system_prompt = st.text_area("System prompt", sys_default, height=80)
    clear_chat = st.button("Clear chat", width="stretch")
    # Load chat(JSON)
    uploaded = st.file_uploader("Load chat (JSON)", type=["json"], key="load_json_sidebar")
    if uploaded is not None:
        try:
            d = json.loads(uploaded.read().decode("utf-8"))
            st.session_state.history = d.get("messages", [])
            st.success("Loaded chat from JSON.")
            st.experimental_rerun()
        except Exception as e:
            st.error(f"Load failed: {e}")
    st.caption("ëª¨ë¸ê³¼ ì„¤ì •ì´ ë°”ë€Œë©´ Reloadë¥¼ ëˆ„ë¥´ì„¸ìš”.")

    # --- Regression bundle (minimal UI ì¶”ê°€) ---
    st.subheader("ğŸ§® Regression bundle")
    if "bundle" not in st.session_state:
        st.session_state.bundle = None
    bundle_path = st.text_input("bundle (.joblib) path", r"c:\Users\BKHOME\mycode\rcmodel\output\stack_bundle_1.joblib", key="bundle_path")
    load_bundle_btn = st.button("Load bundle", width="stretch", key="btn_load_bundle")

    if load_bundle_btn:
        try:
            st.session_state.bundle = load_bundle_cached(bundle_path)
            st.success(f"Bundle loaded: {', '.join(st.session_state.bundle.targets)}")
        except Exception as e:
            st.session_state.bundle = None
            st.error(f"Bundle load failed: {e}")

# ---------- state ----------
if "history" not in st.session_state:
    st.session_state.history = []
if clear_chat:
    st.session_state.history = []
if reload_btn:
    st.cache_resource.clear()

# ---------- model ----------
if not os.path.exists(model_path):
    st.error(f"Model not found: {model_path}")
    st.stop()

chat_format = auto_chat_format(model_path, chat_fmt_choice)
llm = load_llm(model_path, ctx, int(threads), chat_format)

# ---------- UI ----------
st.title("BKChat Local")
for turn in st.session_state.history:
    with st.chat_message(turn["role"]):
        st.markdown(turn["content"])
        # ì €ì¥ ë‹¹ì‹œ expanded ìƒíƒœë¡œë§Œ í‘œì‹œ(ê³¼ê±° ë©”ì‹œì§€ëŠ” í† ê¸€ ë³€ê²½ì˜ ì˜í–¥ì„ ë°›ì§€ ì•ŠìŒ)
        if turn.get("role") == "assistant" and turn.get("think"):
            with st.expander("Show reasoning", expanded=bool(turn.get("expanded", False))):
                st.markdown(turn["think"])

user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”â€¦")
if user_msg:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.session_state.history.append({"role":"user","content":user_msg})
    with st.chat_message("user"):
        st.markdown(user_msg)

    #### ì˜ˆì¸¡ ë§ˆë²•ì‚¬ ìƒíƒœ ì´ˆê¸°í™”
    if "predict_wizard" not in st.session_state:
        st.session_state.predict_wizard = {"active": False, "data": {}}
    
    #### ìì—°ì–´/ëª…ë ¹ì–´ ê¸°ë°˜ ì˜ˆì¸¡ í”Œë¡œìš°
    did_predict = False

    # enable = st.session_state.get("enable_predict")
    bundle_loaded = st.session_state.get("bundle") is not None

    # 0) ë²ˆë“¤ ë¯¸ë¡œë“œ/ë¹„í™œì„± ì‹œ ì•ˆë‚´    
    intent_now = is_predict_intent(user_msg) or (parse_predict_message(user_msg) is not None) or (len(parse_predict_natural(user_msg)) > 0)
    if intent_now and not bundle_loaded:
        with st.chat_message("assistant"):
            st.warning("bundle ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²½ë¡œ í™•ì¸í›„ **Load bundle**.")
        did_predict = True

    # 1) ë§ˆë²•ì‚¬ ì§„í–‰ ì¤‘ì´ë©´ ìˆ˜ì§‘ ê³„ì†    
    if not did_predict and st.session_state.predict_wizard["active"] and bundle_loaded:
        # CLI ë©”ì‹œì§€ì—ëŠ” ìì—°ì–´ íŒŒì„œë¥¼ ì ìš©í•˜ì§€ ì•Šì•„ ê°’ ë®ì–´ì“°ê¸°ë¥¼ ë°©ì§€
        if user_msg.strip().lower().startswith("/predict"):
            d_cli = parse_predict_message(user_msg) or {}
            d_nat = {}
        else:
            d_cli = parse_predict_message(user_msg) or {}
            d_nat = parse_predict_natural(user_msg) or {}
        st.session_state.predict_wizard["data"].update({**d_nat, **d_cli})

        merged = {**d_nat, **d_cli}
        st.session_state.predict_wizard["data"].update(merged)
        # --- debug: ì˜ˆì¸¡ ì…ë ¥ echo ---
        print("[DEBUG] predict_wizard step inputs:", merged)

        collected = st.session_state.predict_wizard["data"]
        missing = [k for k in REQUIRED_INPUT if k not in collected]
        if missing:
            with st.chat_message("assistant"):
                st.info(build_missing_prompt(missing))
            did_predict = True
        else:
            # ëª¨ë“  ì…ë ¥ í™•ë³´ â†’ ì˜ˆì¸¡ ìˆ˜í–‰
            with st.chat_message("assistant"):
                try:
                    preds = st.session_state.bundle.predict_all(collected)
                    # LLM ì„ ì´ìš©í•œ ìì—°ì–´ ìš”ì•½
                    sys_prompt = (
                        "ë„ˆëŠ” êµ¬ì¡°ê³µí•™ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ë³´ê³ í•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
                        "ì£¼ì–´ì§„ ìˆ˜ì¹˜ë¥¼ ë³€ê²½í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ê°„ê²°íˆ ì„¤ëª…í•˜ë¼."
                    )
                    user_prompt = (
                        "ì…ë ¥ê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n"
                        f"- ì…ë ¥: fck={collected.get('fck')} MPa, "
                        f"fy={collected.get('fy')} MPa, "
                        f"width={collected.get('width')} mm, "
                        f"height={collected.get('height')} mm, "
                        f"phi_mn={collected.get('phi_mn')} kNÂ·m\n"
                        f"- ì˜ˆì¸¡: Sm={preds.get('Sm')} mmÂ², "
                        f"bd={preds.get('bd')} mmÂ², "
                        f"rho={preds.get('rho')}, "
                        f"phi_mn={preds.get('phi_mn')} kNÂ·m\n"
                        "í˜•ì‹ ì˜ˆ: 'ë‹¨ë©´ê³„ìˆ˜ Smì€ (), ì² ê·¼ë¹„ rhoëŠ” (), bdëŠ” (), ê³µì¹­íœ¨ê°•ë„ëŠ” () ì…ë‹ˆë‹¤'."
                    )
                    msgs = [
                        {'role':'system', 'content': sys_prompt},
                        {'role':'user',   'content': user_prompt},
                    ]
                    placeholder = st.empty()
                    visible, think = chat_once(llm, msgs, float(temp), float(topp), int(toks), placeholder)
                    placeholder.markdown(visible)
                    st.session_state.history.append({'role':'assistant', 'content': visible})
                except Exception as e:
                    st.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                    st.session_state.history.append({"role":"assistant","content": f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}"})
                finally:
                    st.session_state.predict_wizard = {"active": False, "data": {}}
            did_predict = True

    # 2) ìƒˆ ì…ë ¥ìœ¼ë¡œ ë§ˆë²•ì‚¬ ì‹œì‘ ì—¬ë¶€ ê²°ì •    
    if not did_predict and bundle_loaded:
        if user_msg.strip().lower().startswith("/predict"):
            d_cli0 = parse_predict_message(user_msg or "") or {}
            d_nat0 = {}
        else:
            d_cli0 = parse_predict_message(user_msg or "") or {}
            d_nat0 = parse_predict_natural(user_msg or "") or {}
        # trigger = (d_cli0 is not None) or (len(d_nat0) > 0)
        trigger = intent_now
        if trigger:
            # dict ë³‘í•©: ìì—°ì–´ > CLI ë¹„ì–´ìˆì„ ê²½ìš° ëŒ€ë¹„
            base = {}
            if isinstance(d_cli0, dict):
                base.update(d_cli0)
            base.update(d_nat0)
            
            # --- debug: ìµœì´ˆ ì˜ˆì¸¡ ì…ë ¥ echo ---
            print("[DEBUG] initial predict inputs:", base)
            missing = [k for k in REQUIRED_INPUT if k not in base]

            if missing:
                st.session_state.predict_wizard = {"active": True, "data": base}
                with st.chat_message("assistant"):
                    st.info(build_missing_prompt(missing))
                did_predict = True
            else:
                with st.chat_message('assistant'):
                    try:
                        preds = st.session_state.bundle.predict_all(base)
                        sys_prompt = (
                            "ë„ˆëŠ” êµ¬ì¡°ê³µí•™ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ë³´ê³ í•˜ëŠ” ë„ìš°ë¯¸ë‹¤. "
                            "ì£¼ì–´ì§„ ìˆ˜ì¹˜ë¥¼ ë³€ê²½í•˜ì§€ ë§ê³  ê·¸ëŒ€ë¡œ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ê°„ê²°íˆ ì„¤ëª…í•˜ë¼."
                        )
                        user_prompt = (
                            "ì…ë ¥ê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.\n"
                            f"- ì…ë ¥: fck={base.get('fck')} MPa, "
                            f"fy={base.get('fy')} MPa, "
                            f"width={base.get('width')} mm, "
                            f"height={base.get('height')} mm, "
                            f"phi_mn={base.get('phi_mn')} kNÂ·m\n"
                            f"- ì˜ˆì¸¡: Sm={preds.get('Sm')} mmÂ², "
                            f"bd={preds.get('bd')} mmÂ², "
                            f"rho={preds.get('rho')}, "
                            f"phi_mn={preds.get('phi_mn')} kNÂ·m\n"
                            "í˜•ì‹ ì˜ˆ: 'ë‹¨ë©´ê³„ìˆ˜ Smì€ (), ì² ê·¼ë¹„ rhoëŠ” (), bdëŠ” (), ê³µì¹­íœ¨ê°•ë„ëŠ” () ì…ë‹ˆë‹¤'."
                        )
                        msgs = [
                            {'role':'system', 'content': sys_prompt},
                            {'role':'user',   'content': user_prompt},
                        ]
                        placeholder = st.empty()
                        visible, think = chat_once(llm, msgs, float(temp), float(topp), int(toks), placeholder)
                        placeholder.markdown(visible)
                        st.session_state.history.append({'role':'assistant', 'content': visible})
                    except Exception as e:
                        st.error(f'ì˜ˆì¸¡ ì‹¤íŒ¨: {e}')
                        st.session_state.history.append({"role":"assistant","content": f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}"})
                        
                did_predict = True

    # ëª¨ë¸ ì‘ë‹µ (ì˜ˆì¸¡ í”Œë¡œìš°ê°€ ì•„ë‹ˆê±°ë‚˜ ì¢…ë£Œëœ ê²½ìš°)
    if not did_predict:
        with st.chat_message("assistant"):
            placeholder = st.empty()
            msgs = [{"role":"system","content":system_prompt}, *st.session_state.history]
            visible, think = chat_once(llm, msgs, float(temp), float(topp), int(toks), placeholder)
            expanded_now = not hide_think_default
            placeholder.markdown(visible)
            if think:
                with st.expander("Show reasoning", expanded=expanded_now):
                    st.markdown(think)
            st.session_state.history.append({
                "role":"assistant",
                "content": visible,
                "think": think,
                "expanded": expanded_now
            })

# ë‹¤ìš´/ì—…ë¡œë“œ(ëŒ€í™” ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°)
if st.button("Save chat"):
    data = {"system": system_prompt, "messages": st.session_state.history}
    st.download_button("Download JSON",
                       data=json.dumps(data, ensure_ascii=False, indent=2),
                       file_name=f"chat_{int(time.time())}.json",
                       mime="application/json",
                       use_container_width=True)
